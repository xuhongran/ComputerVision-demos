{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#     This program is free software: you can redistribute it and/or modify\n",
    "#     it under the terms of the GNU General Public License as published by\n",
    "#     the Free Software Foundation, either version 3 of the License, or\n",
    "#     (at your option) any later version.\n",
    "#\n",
    "#     This program is distributed in the hope that it will be useful,\n",
    "#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#     GNU General Public License for more details.\n",
    "#\n",
    "#     You should have received a copy of the GNU General Public License\n",
    "#     along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "#     Written by Charalambos (Charis) Poullis - www.poullis.org"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "#progress bar functionality\n",
    "import tqdm\n",
    "#plotting etc\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#random number generator\n",
    "import random\n",
    "\n",
    "# There is a weird crash in Python 3.9 causing the kernel to restart when using matplotlib\n",
    "# To test if you have the same issue uncomment the following command. If it works, then you don't need the subsequent 2 lines (import os, KMP)\n",
    "#plt.subplot()\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ca187e-a47f-4e84-b2b7-2d572497ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Training dataset: Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Files already downloaded and verified\n",
      "Testing dataset: Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "#Set the path for the datasets\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "#Download the training dataset: CIFAR10\n",
    "training_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH,\n",
    "                                                train=True,\n",
    "                                                download=True,\n",
    "                                                transform=torchvision.transforms.ToTensor())\n",
    "print('Training dataset:', training_dataset)\n",
    "\n",
    "#Download the testing dataset: CIFAR10\n",
    "testing_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH,\n",
    "                                               train=False,\n",
    "                                               download=True,\n",
    "                                               transform=torchvision.transforms.ToTensor())\n",
    "print('Testing dataset:', testing_dataset)  \n",
    "\n",
    "#Create a list with user-friendly names for each label\n",
    "labels=[\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261c381b-5e3e-4cfd-8bc1-c8d841d1f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes a CIFAR dataset and splits it into two numpy arrays containing\n",
    "#the images and labels, respectively\n",
    "\n",
    "def splitDataset(dataset):\n",
    "    #Divide the datasets into images and labels\n",
    "    x, y = dataset[0]\n",
    "    \n",
    "    shape = [(i, *j) for i, j in [(len(dataset), x.shape)]][0]\n",
    "    images = np.zeros(shape)\n",
    "    labels = np.zeros((len(dataset), 1))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        x,y = dataset[i]\n",
    "        images[i] = x\n",
    "        labels[i] = y\n",
    "        \n",
    "    images = np.reshape(images, (len(dataset), 3, 32, 32))\n",
    "    labels = np.reshape(labels, (len(dataset), 1))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fc3be3-a767-4743-a4c3-cbd8a9d07625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 50000, Training labels: 50000\n",
      "Testing images: 10000, Testing labels: 10000\n"
     ]
    }
   ],
   "source": [
    "#Divide the training datasets into images and labels; this is needed later to randomly index from the arrays\n",
    "training_images, training_labels = splitDataset(training_dataset)\n",
    "#Do the same for the testing dataset\n",
    "testing_images, testing_labels = splitDataset(testing_dataset)   \n",
    "\n",
    "print(f'Training images: {len(training_images)}, Training labels: {len(training_labels)}')\n",
    "print(f'Testing images: {len(testing_images)}, Testing labels: {len(testing_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acdcd13-419f-41c4-ba95-35551e6a34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the autoencoder; This is a simple autoencoder with 2 convolutional layers in the encoder, and 2 convolutional layers in the decoder\n",
    "#Put simply, an autoencoder learns to reconstruct the input\n",
    "class MyAutoencoder (torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyAutoencoder, self).__init__()\n",
    "        \n",
    "        #This list will keep each layer of the model\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            #Start of encoding\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4), #[batch_size, 32, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=8, kernel_size=3, padding=2), #[batch_size, 8, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            #End of encoding\n",
    "            #Start of decoding\n",
    "            torch.nn.ConvTranspose2d(in_channels=8, out_channels=32, kernel_size=3, padding=2), #[batch_size, 8, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, padding=4), #[batch_size, 32, 32, 32]\n",
    "            #End of encoding\n",
    "            #At this point the range of the output value is unbound. Let's use Maps the value to the range [0,1] which is expected for colors\n",
    "            torch.nn.Sigmoid() \n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Feed-forward the input through all the layers\n",
    "        result = self.layers(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf991d5-1c58-4e29-9f2c-2f519c60d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices found: 0\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "#We'll use GPU is available; check if a GPU is available\n",
    "print(f'GPU devices found: {torch.cuda.device_count()}')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715e9008-fa84-4fad-a894-6ac27437158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyAutoencoder(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4))\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the model and print; it should display all the layers and their related information\n",
    "model = MyAutoencoder().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6b930c-8afa-4c2f-941c-49b8b65900e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[0.4742, 0.4751, 0.4758,  ..., 0.4725, 0.4743, 0.4754],\n          [0.4748, 0.4743, 0.4753,  ..., 0.4665, 0.4689, 0.4692],\n          [0.4750, 0.4747, 0.4763,  ..., 0.4639, 0.4656, 0.4673],\n          ...,\n          [0.4716, 0.4656, 0.4627,  ..., 0.4609, 0.4570, 0.4618],\n          [0.4695, 0.4663, 0.4638,  ..., 0.4627, 0.4604, 0.4632],\n          [0.4696, 0.4679, 0.4622,  ..., 0.4664, 0.4644, 0.4652]],\n\n         [[0.4932, 0.4960, 0.4952,  ..., 0.4953, 0.4948, 0.4948],\n          [0.4923, 0.4954, 0.4960,  ..., 0.4931, 0.4925, 0.4915],\n          [0.4912, 0.4945, 0.4964,  ..., 0.4894, 0.4883, 0.4881],\n          ...,\n          [0.4932, 0.4987, 0.4958,  ..., 0.4928, 0.4929, 0.4891],\n          [0.4944, 0.4973, 0.4985,  ..., 0.4963, 0.4966, 0.4932],\n          [0.4943, 0.4944, 0.4942,  ..., 0.4891, 0.4913, 0.4909]],\n\n         [[0.5092, 0.5086, 0.5088,  ..., 0.5019, 0.5012, 0.5012],\n          [0.5101, 0.5095, 0.5092,  ..., 0.5002, 0.5007, 0.4996],\n          [0.5117, 0.5108, 0.5104,  ..., 0.5023, 0.5004, 0.5005],\n          ...,\n          [0.5101, 0.5000, 0.4970,  ..., 0.4957, 0.4946, 0.4973],\n          [0.5101, 0.5035, 0.5018,  ..., 0.5007, 0.4961, 0.5014],\n          [0.5107, 0.5043, 0.5025,  ..., 0.5040, 0.5021, 0.5042]]]],\n       grad_fn=<SigmoidBackward>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SANITY CHECK: Feed forward an image and check for errors\n",
    "#Pull the first image from the training images, and convert to a float tensor\n",
    "img = torch.from_numpy(training_images[0]).float()\n",
    "print(img.shape)\n",
    "\n",
    "#The input to the autoencoder is [batch_size, 3, 32, 32]; convert the image's shape [3,32,32] to [1,3,32,32] and push to device\n",
    "img = torch.reshape(img, (1, 3, 32, 32)).to(device)\n",
    "\n",
    "#feed forward the image to the network and make sure it works with no errors\n",
    "model.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ce7d29b-bcd9-43e5-b930-29febade850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:   0%|                                                                  | 0/20 [02:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9035/2291991724.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;31m#backpropagate gradients\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;31m#single gradient descent step; update all network parameters based on the gradient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/python397/lib/python3.9/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/python397/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "\n",
    "#define the hyperparameters\n",
    "EPOCHS=20\n",
    "LEARNING_RATE=0.1\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Create an optimizer and pass the model's parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Define a loss function; we'll pick mean squared error since we are reconstructing the original\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "#training image indices; [0, 1, 2, 3 ....., 49999]\n",
    "training_dataset_indices = torch.arange(0, len(training_dataset))\n",
    "\n",
    "#keep track of the losses for plotting at the end\n",
    "losses = list()\n",
    "\n",
    "#Create a progress bar the size of the number of epochs\n",
    "status_bar = tqdm.tqdm(range(EPOCHS), ncols=100, desc='loss: ')\n",
    "for i in status_bar:\n",
    "    #At each iteration, perform a permutation of the training_dataset_indices; this ensures that we will use all images and avoid duplication\n",
    "    random_permutation = torch.randperm(training_dataset_indices.size()[0])\n",
    "\n",
    "    #Get BATCH_SIZE number of training images at each epoch; because we sample from an array there will be no duplicates\n",
    "    loss = None\n",
    "    for b in range(0,training_dataset_indices.size()[0], BATCH_SIZE):\n",
    "        #Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the indices of the training images in this batch\n",
    "        batch_indices = random_permutation[b:b+BATCH_SIZE]\n",
    "        #we only need to get the images; the labels are irrelevant in the context of an autoencoder\n",
    "        batch_images = training_images[batch_indices]\n",
    "        #convert to float tensors and push to device\n",
    "        batch_images = torch.from_numpy(batch_images).float().to(device)\n",
    "\n",
    "        #feed forward through the network's layers and get the reconstruction\n",
    "        reconstructions = model.forward(batch_images)\n",
    "        \n",
    "        #Calculate the MSE between the reconstructions and the input images; the optimal value for this is 0\n",
    "        loss = loss_function(reconstructions, batch_images)\n",
    "        \n",
    "        #keep track of the losses per batch, per epoch it to the list; used for plotting later on\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        #backpropagate gradients\n",
    "        loss.backward()\n",
    "        #single gradient descent step; update all network parameters based on the gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "    #After every epoch update the loss function\n",
    "    status_bar.set_description(f'loss: {loss.item():.8f}')\n",
    "\n",
    "#Output the final loss on the training dataset\n",
    "print(f'Final loss on training: {loss.item():.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1cb769-3ab0-4598-8264-70c4a089d73b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10000,) and (282,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9035/2943279331.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#Plot the loss as a function of epochs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubplots\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_dataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_title\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Loss on training dataset'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xlabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'epoch'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36mplot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1603\u001B[0m         \"\"\"\n\u001B[1;32m   1604\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize_kwargs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmlines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLine2D\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1605\u001B[0;31m         \u001B[0mlines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_lines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1606\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1607\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_line\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m    313\u001B[0m                 \u001B[0mthis\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    314\u001B[0m                 \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 315\u001B[0;31m             \u001B[0;32myield\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_plot_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    316\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    317\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_next_color\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36m_plot_args\u001B[0;34m(self, tup, kwargs, return_kwargs)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    500\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 501\u001B[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001B[0m\u001B[1;32m    502\u001B[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001B[1;32m    503\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (10000,) and (282,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss as a function of epochs\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(list(range(EPOCHS*int(len(training_dataset)/BATCH_SIZE))), np.array(losses))\n",
    "ax.set_title('Loss on training dataset')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()\n",
    "fig.savefig(\"autoencoder_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce6f9a-8733-4407-8d79-dcd54dbca7ae",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Calculate MSE on the testing dataset; convert to float tensors and push to device; I'm passing all the images together since it fits in the GPU\n",
    "testing_images = torch.from_numpy(testing_images).float().reshape( (len(testing_images), 3, 32, 32) ).to(device)\n",
    "#get the reconstructions\n",
    "reconstructions = model.forward(testing_images)\n",
    "testing_loss = loss_function(reconstructions, testing_images).item()\n",
    "#Output the MSE loss\n",
    "print(f'MSE loss on testing: {testing_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63ae0-8176-4921-871f-577e1ceba0c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Plot a couple of examples\n",
    "cols, rows = 2, 10\n",
    "image_counter = 1\n",
    "figure = plt.figure(figsize=(3*cols, 3*rows))\n",
    "for i in range(10):\n",
    "    #get a random index\n",
    "    rand_index = random.randint(0, len(testing_images))\n",
    "    #get the original image, pull back to cpu and convert to numpy\n",
    "    original_img = testing_images[rand_index].permute(1,2,0).cpu().detach().numpy()\n",
    "    #get the reconstructed image, pull back to cpu and convert to numpy\n",
    "    reconstructed_img = reconstructions[rand_index].permute(1,2,0).cpu().detach().numpy()\n",
    "    \n",
    "    #Add a figure for the original image\n",
    "    figure.add_subplot(rows, cols, image_counter)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(original_img)\n",
    "    image_counter+=1\n",
    "\n",
    "    #Add a figure for the reconstructed image\n",
    "    figure.add_subplot(rows, cols, image_counter)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(reconstructed_img)\n",
    "    image_counter+=1\n",
    "\n",
    "plt.show()\n",
    "figure.savefig(\"autoencoder_reconstruction.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}