{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#     This program is free software: you can redistribute it and/or modify\n",
    "#     it under the terms of the GNU General Public License as published by\n",
    "#     the Free Software Foundation, either version 3 of the License, or\n",
    "#     (at your option) any later version.\n",
    "#\n",
    "#     This program is distributed in the hope that it will be useful,\n",
    "#     but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#     GNU General Public License for more details.\n",
    "#\n",
    "#     You should have received a copy of the GNU General Public License\n",
    "#     along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "#     Written by Charalambos (Charis) Poullis - www.poullis.org"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "#progress bar functionality\n",
    "import tqdm\n",
    "#plotting etc\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#random number generator\n",
    "import random\n",
    "\n",
    "# There is a weird crash in Python 3.9 causing the kernel to restart when using matplotlib\n",
    "# To test if you have the same issue uncomment the following command. If it works, then you don't need the subsequent 2 lines (import os, KMP)\n",
    "#plt.subplot()\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca187e-a47f-4e84-b2b7-2d572497ea09",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Set the path for the datasets\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "#Download the training dataset: CIFAR10\n",
    "training_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH,\n",
    "                                                train=True,\n",
    "                                                download=True,\n",
    "                                                transform=torchvision.transforms.ToTensor())\n",
    "print('Training dataset:', training_dataset)\n",
    "\n",
    "#Download the testing dataset: CIFAR10\n",
    "testing_dataset = torchvision.datasets.CIFAR10(root=DATASET_PATH,\n",
    "                                               train=False,\n",
    "                                               download=True,\n",
    "                                               transform=torchvision.transforms.ToTensor())\n",
    "print('Testing dataset:', testing_dataset)  \n",
    "\n",
    "#Create a list with user-friendly names for each label\n",
    "labels=[\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c381b-5e3e-4cfd-8bc1-c8d841d1f6c7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#This function takes a CIFAR dataset and splits it into two numpy arrays containing\n",
    "#the images and labels, respectively\n",
    "\n",
    "def splitDataset(dataset):\n",
    "    #Divide the datasets into images and labels\n",
    "    x, y = dataset[0]\n",
    "    \n",
    "    shape = [(i, *j) for i, j in [(len(dataset), x.shape)]][0]\n",
    "    images = np.zeros(shape)\n",
    "    labels = np.zeros((len(dataset), 1))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        x,y = dataset[i]\n",
    "        images[i] = x\n",
    "        labels[i] = y\n",
    "        \n",
    "    images = np.reshape(images, (len(dataset), 3, 32, 32))\n",
    "    labels = np.reshape(labels, (len(dataset), 1))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc3be3-a767-4743-a4c3-cbd8a9d07625",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Divide the training datasets into images and labels; this is needed later to randomly index from the arrays\n",
    "training_images, training_labels = splitDataset(training_dataset)\n",
    "#Do the same for the testing dataset\n",
    "testing_images, testing_labels = splitDataset(testing_dataset)   \n",
    "\n",
    "print(f'Training images: {len(training_images)}, Training labels: {len(training_labels)}')\n",
    "print(f'Testing images: {len(testing_images)}, Testing labels: {len(testing_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdcd13-419f-41c4-ba95-35551e6a34ca",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Define the autoencoder; This is a simple autoencoder with 2 convolutional layers in the encoder, and 2 convolutional layers in the decoder\n",
    "#Put simply, an autoencoder learns to reconstruct the input\n",
    "class MyAutoencoder (torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyAutoencoder, self).__init__()\n",
    "        \n",
    "        #This list will keep each layer of the model\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            #Start of encoding\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=4), #[batch_size, 32, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=8, kernel_size=3, padding=2), #[batch_size, 8, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            #End of encoding\n",
    "            #Start of decoding\n",
    "            torch.nn.ConvTranspose2d(in_channels=8, out_channels=32, kernel_size=3, padding=2), #[batch_size, 8, 32, 32]\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=5, padding=4), #[batch_size, 32, 32, 32]\n",
    "            #End of encoding\n",
    "            #At this point the range of the output value is unbound. Let's use Maps the value to the range [0,1] which is expected for colors\n",
    "            torch.nn.Sigmoid() \n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Feed-forward the input through all the layers\n",
    "        result = self.layers(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf991d5-1c58-4e29-9f2c-2f519c60d4a0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#We'll use GPU is available; check if a GPU is available\n",
    "print(f'GPU devices found: {torch.cuda.device_count()}')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e9008-fa84-4fad-a894-6ac27437158d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Create an instance of the model and print; it should display all the layers and their related information\n",
    "model = MyAutoencoder().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b930c-8afa-4c2f-941c-49b8b65900e5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#SANITY CHECK: Feed forward an image and check for errors\n",
    "#Pull the first image from the training images, and convert to a float tensor\n",
    "img = torch.from_numpy(training_images[0]).float()\n",
    "print(img.shape)\n",
    "\n",
    "#The input to the autoencoder is [batch_size, 3, 32, 32]; convert the image's shape [3,32,32] to [1,3,32,32] and push to device\n",
    "img = torch.reshape(img, (1, 3, 32, 32)).to(device)\n",
    "\n",
    "#feed forward the image to the network and make sure it works with no errors\n",
    "model.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7d29b-bcd9-43e5-b930-29febade850d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "#define the hyperparameters\n",
    "EPOCHS=20\n",
    "LEARNING_RATE=0.1\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "#Create an optimizer and pass the model's parameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Define a loss function; we'll pick mean squared error since we are reconstructing the original\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "#training image indices; [0, 1, 2, 3 ....., 49999]\n",
    "training_dataset_indices = torch.arange(0, len(training_dataset))\n",
    "\n",
    "#keep track of the losses for plotting at the end\n",
    "losses = list()\n",
    "\n",
    "#Create a progress bar the size of the number of epochs\n",
    "status_bar = tqdm.tqdm(range(EPOCHS), ncols=100, desc='loss: ')\n",
    "for i in status_bar:\n",
    "    #At each iteration, perform a permutation of the training_dataset_indices; this ensures that we will use all images and avoid duplication\n",
    "    random_permutation = torch.randperm(training_dataset_indices.size()[0])\n",
    "\n",
    "    #Get BATCH_SIZE number of training images at each epoch; because we sample from an array there will be no duplicates\n",
    "    loss = None\n",
    "    for b in range(0,training_dataset_indices.size()[0], BATCH_SIZE):\n",
    "        #Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the indices of the training images in this batch\n",
    "        batch_indices = random_permutation[b:b+BATCH_SIZE]\n",
    "        #we only need to get the images; the labels are irrelevant in the context of an autoencoder\n",
    "        batch_images = training_images[batch_indices]\n",
    "        #convert to float tensors and push to device\n",
    "        batch_images = torch.from_numpy(batch_images).float().to(device)\n",
    "\n",
    "        #feed forward through the network's layers and get the reconstruction\n",
    "        reconstructions = model.forward(batch_images)\n",
    "        \n",
    "        #Calculate the MSE between the reconstructions and the input images; the optimal value for this is 0\n",
    "        loss = loss_function(reconstructions, batch_images)\n",
    "        \n",
    "        #keep track of the losses per batch, per epoch it to the list; used for plotting later on\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        #backpropagate gradients\n",
    "        loss.backward()\n",
    "        #single gradient descent step; update all network parameters based on the gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "    #After every epoch update the loss function\n",
    "    status_bar.set_description(f'loss: {loss.item():.8f}')\n",
    "\n",
    "#Output the final loss on the training dataset\n",
    "print(f'Final loss on training: {loss.item():.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cb769-3ab0-4598-8264-70c4a089d73b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Plot the loss as a function of epochs\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(list(range(EPOCHS*int(len(training_dataset)/BATCH_SIZE))), np.array(losses))\n",
    "ax.set_title('Loss on training dataset')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()\n",
    "fig.savefig(\"autoencoder_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce6f9a-8733-4407-8d79-dcd54dbca7ae",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Calculate MSE on the testing dataset; convert to float tensors and push to device; I'm passing all the images together since it fits in the GPU\n",
    "testing_images = torch.from_numpy(testing_images).float().reshape( (len(testing_images), 3, 32, 32) ).to(device)\n",
    "#get the reconstructions\n",
    "reconstructions = model.forward(testing_images)\n",
    "testing_loss = loss_function(reconstructions, testing_images).item()\n",
    "#Output the MSE loss\n",
    "print(f'MSE loss on testing: {testing_loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63ae0-8176-4921-871f-577e1ceba0c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Plot a couple of examples\n",
    "cols, rows = 10, 2\n",
    "image_counter = 1\n",
    "figure = plt.figure(figsize=(3*cols, 3*rows))\n",
    "for i in range(10):\n",
    "    #get a random index\n",
    "    rand_index = random.randint(0, len(testing_images))\n",
    "    #get the original image, pull back to cpu and convert to numpy\n",
    "    original_img = testing_images[rand_index].permute(1,2,0).cpu().detach().numpy()\n",
    "    #get the reconstructed image, pull back to cpu and convert to numpy\n",
    "    reconstructed_img = reconstructions[rand_index].permute(1,2,0).cpu().detach().numpy()\n",
    "    \n",
    "    #Add a figure for the original image\n",
    "    figure.add_subplot(rows, cols, image_counter)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(original_img)\n",
    "\n",
    "    #Add a figure for the reconstructed image\n",
    "    figure.add_subplot(rows, cols, image_counter+10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(reconstructed_img)\n",
    "    image_counter+=1\n",
    "\n",
    "plt.show()\n",
    "figure.savefig(\"autoencoder_reconstruction.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}